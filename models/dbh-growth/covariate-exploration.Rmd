---
title: "DBH Growth Model for the Northern Forest"
subtitle: "Exploration of Covariates"
author: "Neal Maker"
output: html_notebook
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE)

library("tidyverse")
library("caret")
library("recipes")
```

# Goals

1. See if dbh-height ratio could replace cr, which might make long-term testing with existing datasets easier. (It could also be a better measure of past vigor or better proxy for age.)

2. Check out some other more innovative covariates (such as Aaron suggested). These could include structural diversity metrics and species diversity metrics, which would pair well with recent research into multi-age and mult-species management. (Aaron mentioned climate factors, species functional traits like specific gravity and shade tolerance, species and structural diversity indexes, and more sophisticated measures of competition like SDI.)

3. Explore importance of species specific ba and bal.

  a. what is difference with and without them, all else equal?
  
  b. Do we lose anything by just using one of them (which would save calculations in Forest Maker)?
  
## New Covariates to Try

Subplot or plot level covariates need to be calculated in "nf-dataset-creation.Rmd".

- dbh-height ratio (calculated)

- height (don't calculate ratio, but algo might find it on its own)
  
- better, custom competition index (subplot level; based on estimated crown volumes, estimated light interception, and difference in height from target tree)

- spp diversity (subplot level)

  With random samples of stands, would probably use hill diversity and standardize coverage between stands [see Roswell et al. 2021]. Alternately, could use Meninick's or Margalefs indexes for richness and Gini-Simpson's or Shannon-Wiener indexes for eveness [Simpsons might be more statistically appropriate, but Shannon is mores sensitive to richness, which is important].
  
  Big problem here is we're interested in diversity in single plots (n=1). Above metrics don't work without a real sample. I think all we could do is consider individual plots the units of interest (not the larger stands they're in) and calculate species richness directly (since we have a census of each plot). The assumption here would be that a tree's growth is only influenced by the diversity of the 1/24th acre neighborhood it's in and not by the diversity of the larger stand (probably not too unreasonable an assumption).   
  
- structural diversity (subplot or plot level)

  - coefficient of variation of heights (sd/mean, see Brassard et al., 2008), or use spp eveness index with size classes (Buongiorno et al., 1994; Dănescu et al., 2016; Magurran, 2004; Staudhammer and LeMay, 2001; but probably can't because of lack of sample)
  
  - coefficient of variation of dbh (sd/mean, see Brassard et al., 2008), or use spp eveness index with size classes (Buongiorno et al., 1994; Dănescu et al., 2016; Magurran, 2004; Staudhammer and LeMay, 2001; but probably can't because of lack of sample)

# Data

```{r fetch, cache=TRUE}
invisible({capture.output({

temp <- tempfile()
download.file(
  "https://github.com/nealmaker/tree-growth-nf/raw/master/data/nf-fia.rda",
  temp)
load(temp)

# NEED TO CALCULATE PLOT METRICS IN DATASET CREATION SCRIPT, AS nf_fia DOESN"T
# NECCESARILY CONTAIN ALL TREES IN EACH PLOT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# remove trees that died and unwanted variables
nf_fia <- nf_fia %>%
  filter(status_change == "lived",
         !is.na(psi_mid), !is.na(ht_mid), !is.na(dbh_var_mid)) %>%
  select(dbh_rate, spp, dbh_mid, cr_mid, ht_mid, crown_class_s, tree_class_s,
         ba_mid, bal_mid, psi_mid, spp_rich_mid, dbh_var_mid, ht_var_mid, 
         forest_type_s, stocking_s, landscape,
         site_class, slope, aspect, lat, lon, elev, ba_ash:psi_yellow.birch,
         interval, plot) %>%
  rename(dbh = dbh_mid, cr = cr_mid, ht = ht_mid, crown_class = crown_class_s,
         tree_class = tree_class_s, ba = ba_mid, bal = bal_mid, psi = psi_mid, 
         spp_rich = spp_rich_mid, dbh_var = dbh_var_mid, ht_var = ht_var_mid, 
         forest_type = forest_type_s, stocking = stocking_s)
})})
```

Forest Inventory and Analysis data were used from the US Northern Forest region. For this analysis, only data that includes measures of porous sail index (our new competition index) and height were used. They consist of observations of `r nrow(nf_fia)` remeasured trees, located in `r length(unique(nf_fia[, ncol(nf_fia)]))` unique sample plots. Details are available at the project's [GitHub page](https://github.com/nealmaker/tree-growth-nf/blob/master/fia/nf-dataset-creation.pdf).

Preliminary analysis using [nonlinear least squares models](https://github.com/nealmaker/tree-growth-nf/blob/master/models/dbh-growth/nls-model.Rmd) demonstrated the value of using the annualized diameter growth rate as a response variable, instead of using the change in diameter over the entire remeasurement period. For this reason, the annualized diameter at breast height ($DBH\_rate$) was used, and mid-period values were used for mutable individual tree attributes to better reflect the average conditions over the remeasurement period. These were calculated as the average of the starting and ending values.

The data include `r ncol(nf_fia) - 2` potential predictors (features), which include individual tree attributes as well as plot-level attributes and site attributes. Three of the predictors are non-ordinal categorical variables, three are ordinal categorical variables, and the rest are numeric. Many of the numeric predictors are species-specific basal areas, species-specific overtopping basal areas and species-specific porous sail indexes.  

```{r split, cache=TRUE}
# test set is 20% of full dataset
test_size <- .2

# define test set based on plots (to make it truly independent)
set.seed(10)
test_plots <- sample(unique(nf_fia$plot),
                     size = round(test_size * length(unique(nf_fia$plot))),
                     replace = FALSE)

index <- which(nf_fia$plot %in% test_plots)

# remove plots and intervals, which are no longer needed
nf_fia <- select(nf_fia, -plot, -interval)

train <- nf_fia[-index, ]
test <- nf_fia[index, ]
```

Data from `r 100 * test_size`% of all the sample plots was set aside for testing. Data were split based on plots to make sure that the testing data was truly independent. The data from the remaining `r 100 * (1 - test_size)`% of plots were used to explore different algorithms and to train the final model. These testing and training sets are the same that were used in training and testing a nonlinear least squares model previously, and as with the nonlinear least squares model, the training data was not used in this analysis so that the model created here can be compared to others without data leakage. 


# Preprocessing

Preprocessing was performed using recipes, to allow for easy reproducibility within cross validation without data leakage. Recipes allow preprocessing steps to be prescribed ahead of time, then easily applied to different data. For example, a recipe can call for centering a certain feature, but the centering is actually done fold by fold during cross validation, so that each fold is treated independently.

A number of preprocessing steps were carried out on the covariates to ensure that data would be suitable for a wide range of modeling methods and to make for more accurate models. 

Nominal categorical variables were re-coded using dummy encoding, to allow for their incorporation into various regression models. Dummy encoding was used instead of one-hot encoding to avoid collinearity. Several ordinal categorical variables are present in the data too, but they are already encoded with appropriate integers, so no preproceesing of them was needed.

Numeric predictors were normalized using Yeo-Johnson transformation and were centered and scaled to put them in similar units. This can help improve the accuracy of many models, and can alleviate problems with parametric models that work better with normally distributed covariates. Yeo-Johnson transformation was chosen because some of the features contain negative values. 

A test was done using Random forest algorithms to assess the effect of standardizing and normalizing dummy-encoded variables and the sparse, species-specific basal areas, overtopping basal areas and porous sail indexes.

```{r testOptions, cache=TRUE}
# Cross validation
control <- trainControl(method = "cv", number = 10)
seed <- 7
metric <- "RMSE"
```

```{r preprocTest}
# Recipes with and without standardizing dummies 
dbh_recipe1 <- recipe(dbh_rate ~ ., data = train) %>% 
  step_YeoJohnson("dbh", "cr", "crown_class", "tree_class", "stocking",
                  "site_class", "slope", "aspect", "lat", "lon", "elev",
                  "spp_rich", "dbh_var", "ht_var") %>% 
  step_center("dbh", "cr", "crown_class", "tree_class", "stocking",
                  "site_class", "slope", "aspect", "lat", "lon", "elev",
                  "spp_rich", "dbh_var", "ht_var") %>% 
  step_scale("dbh", "cr", "crown_class", "tree_class", "stocking",
                  "site_class", "slope", "aspect", "lat", "lon", "elev",
                  "spp_rich", "dbh_var", "ht_var") %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe2 <- recipe(dbh_rate ~ ., data = train) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default
  
# Train both ways w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.nodum <- train(dbh_recipe1, train, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.dum <- train(dbh_recipe2, train, method = 'ranger', 
                   metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results <- resamples(list(standardized = fit.dum, not = fit.nodum))
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales = scales)
```
Because the standardization and normalization didn't affect accuracy in any meaningful way and were slightly faster than leaving dummy-encoded and sparse variables un-standardized and abnormal, standardization and normalization were used on all numeric variables going forward. 

# Effects of Covariates

To start, a random forest model of dbh rate was constructed using all of the available covariates except for the species specific competition metrics (which are explored in more depth later). Variable importance scores were calculated to get a sense of which covariates are the most useful overall.

```{r varimp}
train_varimp <- train[, 1:22]

dbh_recipe0 <- recipe(dbh_rate ~ ., data = train_varimp) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.varimp <- train(dbh_recipe0, train_varimp, method = 'ranger',
                      importance = "permutation", metric = metric, 
                      trControl = control)
  
  stopCluster(cl)
})})

mod_varimp <- varImp(fit.varimp)$importance
mod_varimp <- tibble(covar = row.names(mod_varimp), 
                     importance = mod_varimp$Overall)
spp_vi <- 
  tibble(covar = "spp", importance = sum(mod_varimp$importance[
           which(str_detect(mod_varimp$covar, "spp_"))]))
for_vi <- tibble(covar = "forest_type", importance = 
                   sum(mod_varimp$importance[
                     which(str_detect(mod_varimp$covar, "forest_type_"))]))
land_vi <- tibble(covar = "landscape", importance = 
                   sum(mod_varimp$importance[
                     which(str_detect(mod_varimp$covar, "landscape_"))]))
mod_varimp[1:18, ] %>% 
  rbind(spp_vi, for_vi, land_vi) %>% 
  mutate(covar = fct_reorder(covar, importance)) %>% 
  ggplot(aes(importance, covar)) +
  geom_segment(aes(x = 0, xend = importance, y = covar, yend = covar)) +
  geom_point(size = 4)
```

## DBH to Height Ratio

To address the first goal, three models were compared that were identical except that one used crown ratio without dbh to height ratio or height as covariates, the second used crown ratio and height, but not dbh to height ratio, and the third used dbh to height ratio without crown ratio. Only four other covariates were used in these models (species, dbh, ba, and bal) for simplicity.

```{r cr_vs_dbh.ht}
train_cr <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, bal)

train_cr_ht <- train %>% 
  select(dbh_rate, spp, dbh, cr, ht, ba, bal)

train_dbh_ht <- train %>% 
  mutate(dbh_ht = dbh/ht) %>% 
  select(dbh_rate, spp, dbh, dbh_ht, ba, bal)

# Recipes
dbh_recipe3 <- recipe(dbh_rate ~ ., data = train_cr) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe4 <- recipe(dbh_rate ~ ., data = train_cr_ht) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe5 <- recipe(dbh_rate ~ ., data = train_dbh_ht) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.cr <- train(dbh_recipe3, train_cr, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.cr_ht <- train(dbh_recipe4, train_cr_ht, method = 'ranger', 
                   metric=metric, trControl=control)
  set.seed(seed)
  fit.dbh_ht <- train(dbh_recipe5, train_dbh_ht, method = 'ranger', 
                   metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results1 <- resamples(list(cr = fit.cr, cr_ht = fit.cr_ht, dbh_ht = fit.dbh_ht))
bwplot(results1, scales = scales)
```
Results show that dbh to height ratio is a poor substitute for crown ratio. While it pairs better with some of the older datasets out there, doing away with crown ratio would mean a sacrifice in accuracy. When crown ratio is used, the addition of height does seem to improve accuracy, but only slightly.

## Porous Sail Index

The porous sail index is a new competition index, which is described more fully in the [FIA Dataset paper](https://github.com/nealmaker/tree-growth-nf/blob/master/fia/nf-dataset-creation.pdf). It is compared to basal area and overtopping basal area using three separate models. In all of them, species, dbh, and cr are also used. A fourth model is also compared, which uses all three of these competition indexes, to see if they might complement each other.

```{r psi}
train_ba <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba)

train_bal <- train %>% 
  select(dbh_rate, spp, dbh, cr, bal)

train_psi <- train %>% 
  select(dbh_rate, spp, dbh, cr, psi)

train_3comp <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, bal, psi)

# Recipes
dbh_recipe6 <- recipe(dbh_rate ~ ., data = train_ba) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe7 <- recipe(dbh_rate ~ ., data = train_bal) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe8 <- recipe(dbh_rate ~ ., data = train_psi) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe9 <- recipe(dbh_rate ~ ., data = train_3comp) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.ba <- train(dbh_recipe6, train_ba, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.bal <- train(dbh_recipe7, train_bal, method = 'ranger', 
                   metric=metric, trControl=control)
  set.seed(seed)
  fit.psi <- train(dbh_recipe8, train_psi, method = 'ranger', 
                   metric=metric, trControl=control)
  set.seed(seed)
  fit.3comp <- train(dbh_recipe9, train_3comp, method = 'ranger', 
                   metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results2 <- resamples(list(ba = fit.ba, bal = fit.bal, psi = fit.psi, 
                           all = fit.3comp))
bwplot(results2, scales = scales)
```

The three competition indexes all seem to be similarly efficacious on their own, and combining them doesn't offer much improvement--if any--over just using one of them. 

## Species Specific Competition Indexes

Expanding on the competition indexes, the models above were compared to similar models that substitute each index with its suite of species-specific equivalents.

```{r sppSpec}
train_ba_ss <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba_ash:ba_yellow.birch)

train_bal_ss <- train %>% 
  select(dbh_rate, spp, dbh, cr, bal_ash:bal_yellow.birch)

train_psi_ss <- train %>% 
  select(dbh_rate, spp, dbh, cr, psi_ash:psi_yellow.birch)

train_3comp_ss <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, bal, ba_ash:psi_yellow.birch)

# Recipes
dbh_recipe10 <- recipe(dbh_rate ~ ., data = train_ba_ss) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe11 <- recipe(dbh_rate ~ ., data = train_bal_ss) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe12 <- recipe(dbh_rate ~ ., data = train_psi_ss) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe13 <- recipe(dbh_rate ~ ., data = train_3comp_ss) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.ba_ss <- train(dbh_recipe10, train_ba_ss, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.bal_ss <- train(dbh_recipe11, train_bal_ss, method = 'ranger', 
                   metric=metric, trControl=control)
  set.seed(seed)
  fit.psi_ss <- train(dbh_recipe12, train_psi_ss, method = 'ranger', 
                   metric=metric, trControl=control)
  set.seed(seed)
  fit.3comp_ss <- train(dbh_recipe13, train_3comp_ss, method = 'ranger', 
                   metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results3 <- resamples(list(ba = fit.ba, ba_ss = fit.ba_ss, bal = fit.bal, 
                           bal_ss = fit.bal_ss, psi = fit.psi, 
                           psi_ss = fit.psi_ss, all = fit.3comp,
                           all_ss = fit.3comp_ss))
bwplot(results3, scales = scales)
```

Species specific metrics do increase accuracy in a noticeable way, but there are not noticeable gains from using multiple species specific metrics concurrently. If one set of indexes were to be used, the species specific basal areas look to offer the best accuracy with the least computational fuss.

## Diversity

None of the three measures of diversity looked particularly impactful from variable importance scores, but they are explored in more depth here. Models with each on their own were compared, along with a model that uses all three concurrently and another that doesn't use any of them. Like with previous comparisons, only four other covariates were used in these models: species, dbh, crown ratio, and basal area.

```{r diversity}
train_nodiv <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba)

train_rich <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, spp_rich)

train_dbhvar <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, dbh_var)

train_htvar <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, ht_var)

train_3div <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, spp_rich, dbh_var, ht_var)

# Recipes
dbh_recipe14 <- recipe(dbh_rate ~ ., data = train_nodiv) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe15 <- recipe(dbh_rate ~ ., data = train_rich) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe16 <- recipe(dbh_rate ~ ., data = train_dbhvar) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe17 <- recipe(dbh_rate ~ ., data = train_htvar) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe18 <- recipe(dbh_rate ~ ., data = train_3div) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.nodiv <- train(dbh_recipe14, train_nodiv, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.rich <- train(dbh_recipe15, train_rich, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.dbhvar <- train(dbh_recipe16, train_dbhvar, method = 'ranger', 
                   metric=metric, trControl=control)
  set.seed(seed)
  fit.htvar <- train(dbh_recipe17, train_htvar, method = 'ranger', 
                   metric=metric, trControl=control)
  set.seed(seed)
  fit.3div <- train(dbh_recipe18, train_3div, method = 'ranger', 
                   metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results4 <- resamples(list(no_diversity = fit.nodiv, spp_richness = fit.rich, 
                           dbh_variation = fit.dbhvar, 
                           ht_variation = fit.htvar, all = fit.3div))
bwplot(results4, scales = scales)
```

As expected, the diversity measures did not improve accuracy to any great degree. They do appear to be complementary, though, so that using any one of them makes no real difference, but using them together does increase the accuracy somewhat. To follow up, a comparison was made to determine the potential gain of using species richness and dbh variation concurrently, without height variation (since height is difficult to measure and has been shown to be otherwise unimportant).

```{r divNoHt}
train_divnoht <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, spp_rich, dbh_var)

dbh_recipe19 <- recipe(dbh_rate ~ ., data = train_divnoht) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.divnoht <- train(dbh_recipe19, train_divnoht, method = 'ranger', 
                     metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results5 <- resamples(list(no_diversity = fit.nodiv, 
                           richness_and_dbh_var = fit.divnoht, 
                           all_three = fit.3div))
bwplot(results5, scales = scales)
```

Height variability isn't really contributing to the accuracy gains in any noticeable way. If diversity measures are incorporated into a final diameter growth model, it's probably worth using both species richness and diameter variability, omitting height variability.

## Geographic Location

A comparison was also made to determine the importance of geographic location, defined by latitude and longitude. On their own, latitude and longitude look relatively unimportant, but mapping of dbh rates (which shows regions of increased average diameter growth) gives reason to believe that they might be important in concert.

```{r geo}
train_geo <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, lat, lon)

# Recipes
dbh_recipe20 <- recipe(dbh_rate ~ ., data = train_geo) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.geo <- train(dbh_recipe20, train_geo, method = 'ranger', 
                     metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results6 <- resamples(list(no_lat_lon = fit.nodiv, lat_lon = fit.geo))
bwplot(results6, scales = scales)
```

Latitude and longitude do increase the accuracy when used together, despite their lack of importance in isolation.

## Crown Class

Finally, crown class is explored in more depth because it was shown to be relatively unimportant in earlier analysis, but very important in the variable importance scores calculated above. It is not difficult to measure, but it is another thing to measure, and if it is not important it would be a good one to omit. Other measures of competition are used in the this set of models in case crown class interacts with any of them in meaningful ways.

```{r crown_class}
train_nocc <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, bal, ba_ash:ba_yellow.birch)

train_cc <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, bal, ba_ash:ba_yellow.birch, crown_class)

# Recipes
dbh_recipe21 <- recipe(dbh_rate ~ ., data = train_nocc) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe22 <- recipe(dbh_rate ~ ., data = train_cc) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.nocc <- train(dbh_recipe21, train_nocc, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.cc <- train(dbh_recipe22, train_cc, method = 'ranger', 
                   metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results7 <- resamples(list(no_crown_class = fit.nocc, crown_class = fit.cc))
bwplot(results7, scales = scales)
```
The results support the earlier analysis showing crown ratio to be unimportant. It seems the variable importance calculations at the beginning of this working paper should be treated with caution.

# Conclusion

Based on the above analysis, and earlier analyses, I propose the following covariates be incorporated into a "robust" dbh growth model: species, dbh, crown ratio, basal area, species specific basal area, overtopping basal area, species richness, dbh variability, forest type, landscape setting, site class, latitude, and longitude. Site class is included despite its low importance because it's easy to incorporate, so it doesn't really cost anything. Height and stocking are excluded despite being more important because they are hard to measure, and because using height drastically reduces the amount of data available for model training. Forest type and landscape setting are intermediate in that they represent a bit of extra effort (computational expense for forest type and inventory time for landscape setting) for a modest increase in accuracy. 

As a final test of these choices, three models are compared: a minimal model with only the five most important covariates, an 'ideal' model using the covariates identified above, and a 'kitchen sink' model using all the covariates in the dataset.

```{r final}
train_minimal <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, bal)

train_ideal <- train %>% 
  select(dbh_rate, spp, dbh, cr, ba, bal, ba_ash:ba_yellow.birch, spp_rich, 
         dbh_var, forest_type, landscape, site_class, lat, lon)

# Recipes
dbh_recipe23 <- recipe(dbh_rate ~ ., data = train_minimal) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe24 <- recipe(dbh_rate ~ ., data = train_ideal) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe25 <- recipe(dbh_rate ~ ., data = train) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.minimal <- train(dbh_recipe23, train_minimal, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.ideal <- train(dbh_recipe24, train_ideal, method = 'ranger', 
                   metric=metric, trControl=control)
  set.seed(seed)
  fit.ks <- train(dbh_recipe25, train, method = 'ranger', 
                   metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results8 <- resamples(list(minimal = fit.minimal, ideal = fit.ideal,
                           kitchen_sink = fit.ks))
bwplot(results8, scales = scales)
```

At the least, this last comparison does nothing to invalidate the 'ideal' covariate list described above. Despite omitting many of the most computationally and practically expensive covariates, its accuracy is only negligibly lower than the expensive, 'kitchen sink' approach.