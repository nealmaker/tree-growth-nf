---
title: "DBH Growth Model for the Northern Forest"
subtitle: "using a machine learning approach"
author: "Neal Maker"
output: html_notebook
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, warning = FALSE, message = FALSE)

library("tidyverse")
library("caret")
library("recipes")
```

# Goals

1. Develop strong procedures for preprocessing forest inventory data, to allow for efficient diameter growth model training and to improve the accuracy of models developed with the data.  

2. Spot check numerous algorithms for modeling tree diameter growth, which represent a diversity of methodologies. Focus on:
  - model accuracy
  - size of model object (to address memory concerns in simulator)
  - prediction time  

3. Train a 'best case' model for use in simulator

# Data

```{r fetch, cache=TRUE}
invisible({capture.output({

temp <- tempfile()
download.file(
  "https://github.com/nealmaker/tree-growth-nf/raw/master/data/nf-fia.rda",
  temp)
load(temp)

# remove trees that died and unwanted variables
nf_fia <- nf_fia %>%
  filter(status_change == "lived") %>%
  select(dbh_rate, spp, dbh_mid, cr_mid, crown_class_s, tree_class_s,
         forest_type_s, stocking_s, landscape,
         site_class, slope, aspect, lat, lon, elev, ba_ash:bal_yellow.birch,
         interval, plot) %>%
  rename(dbh = dbh_mid, cr = cr_mid, crown_class = crown_class_s,
         tree_class = tree_class_s, forest_type = forest_type_s, stocking = stocking_s)
})})
```

Forest Inventory and Analysis data were used from the US Northern Forest region. They consist of observations of `r nrow(nf_fia)` remeasured trees, located in `r length(unique(nf_fia[, ncol(nf_fia)]))` unique sample plots. Details are available at the project's [GitHub page](https://github.com/nealmaker/tree-growth-nf/blob/master/fia/nf-dataset-creation.pdf).

Preliminary analysis using [nonlinear least squares models](https://github.com/nealmaker/tree-growth-nf/blob/master/models/dbh-growth/nls-model.Rmd) demonstrated the value of using the annualized diameter growth rate as a response variable, instead of using the change in diameter over the entire remeasurement period. For this reason, the annualized diameter at breast height ($DBH\_rate$) was used, and mid-period values were used for mutable individual tree attributes to better reflect the average conditions over the remeasurement period. These were calculated as the average of the starting and ending values.

The data include `r ncol(nf_fia) - 2` potential predictors (features), which include individual tree attributes as well as plot-level attributes and site attributes. Three of the predictors are non-ordinal categorical variables, three are ordinal categorical variables, and the rest are numeric. Many of the numeric predictors are species-specific basal areas and species-specific overtopping basal areas. In aggregate, these fully describe basal area and overtopping basal area for each observation. Total basal area and overtopping basal area were therefore omitted from the covariates to prevent perfect collinearities. 

```{r split, cache=TRUE}
# test set is 20% of full dataset
test_size <- .2

# define test set based on plots (to make it truely independent)
set.seed(10)
test_plots <- sample(unique(nf_fia$plot),
                     size = round(test_size * length(unique(nf_fia$plot))),
                     replace = FALSE)

index <- which(nf_fia$plot %in% test_plots)

# remove plots and intervals, which are no longer needed
nf_fia <- select(nf_fia, -plot, -interval)

train <- nf_fia[-index, ]
test <- nf_fia[index, ]

# sample from training data to expedite algorithm testing
subsamp_size <- 10000
set.seed(201)
subsamp <- sample(1:nrow(train),
                  size = subsamp_size,
                  replace = FALSE)

trainsub <- train[subsamp, ]
```

Data from `r 100 * test_size`% of all the sample plots was set aside for testing. Data were split based on plots to make sure that the testing data was truly independent. The data from the remaining `r 100 * (1 - test_size)`% of plots were used to explore different algorithms and to train the final model. These testing and training sets are the same that were used in training and testing a nonlinear least squares model previously, and as with the nonlinear least squares model, the training data was not used in this analysis so that the model created here can be compared to others without data leakage. 

Spot checking of numerous algorithms would have been very slow on all the training data, so a random sample of `r subsamp_size` observations was drawn from the training data and used instead.

# Preprocessing

Preprocessing was performed using recipes, to allow for easy reproducibility within cross validation without data leakage. Recipes allow preprocessing steps to be prescribed ahead of time, then easily applied to different data. For example, a recipe can call for centering a certain feature, but the centering is actually done fold by fold during cross validation, so that each fold is treated independently.

A number of preprocessing steps were carried out on the covariates to ensure that data would be suitable for a wide range of modeling methods and to make for more accurate models. 

Nominal categorical variables were re-coded using dummy encoding, to allow for their incorporation into various regression models. Dummy encoding was used instead of one-hot encoding to avoid collinearity. Several ordinal categorical variables are present in the data too, but they are already encoded with appropriate integers, so no preproceesing of them was needed.

Numeric predictors were normalized using Yeo-Johnson transformation and were centered and scaled to put them in similar units. This can help improve the accuracy of many models, and can alleviate problems with parametric models that work better with normally distributed covariates. Yeo-Johnson transformation was chosen because some of the features contain negative values. 

A test was done using Random forest algorithms to assess the effect of standardizing and normalizing dummy-encoded variables and the sparse, species-specific basal areas and overtopping basal areas.

```{r testOptions, cache=TRUE}
# Cross validation
control <- trainControl(method = "cv", number = 10)
seed <- 7
metric <- "RMSE"
```

```{r preprocTest}
# Recipes with and without standardizing dummies 
dbh_recipe1 <- recipe(dbh_rate ~ ., data = train) %>% 
  step_YeoJohnson("dbh", "cr", "crown_class", "tree_class", "stocking",
                  "site_class", "slope", "aspect", "lat", "lon", "elev") %>% 
  step_center("dbh", "cr", "crown_class", "tree_class", "stocking",
                  "site_class", "slope", "aspect", "lat", "lon", "elev") %>% 
  step_scale("dbh", "cr", "crown_class", "tree_class", "stocking",
                  "site_class", "slope", "aspect", "lat", "lon", "elev") %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

dbh_recipe2 <- recipe(dbh_rate ~ ., data = train) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_YeoJohnson(all_numeric(), -all_outcomes()) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(), -all_outcomes()) %>% 
  step_zv(all_predictors()) %>% # to remove any dummies with no occurrence 
  step_corr(all_predictors(), threshold = 0.95) # Pearson's corr is default

# Train both ways w/ RF
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.nodum <- train(dbh_recipe1, trainsub, method = 'ranger', 
                     metric=metric, trControl=control)
  set.seed(seed)
  fit.dum <- train(dbh_recipe2, trainsub, method = 'ranger', 
                   metric=metric, trControl=control)
  
  stopCluster(cl)
})})

# Compare
results <- resamples(list(standardized = fit.dum, not = fit.nodum))
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales = scales)
```

Standardization and normalization of the sparse and binary (dummy-encoded) variables looks to have a negligible effect on model accuracy, and these steps were left out of the preprocessing.

```{r preproc}
dbh_recipe <- dbh_recipe1
rm(fit.dum, fit.nodum, dbh_recipe1, dbh_recipe2)
```

Finally, highly correlated predictors were filtered, to cut down on features to speed training and prevent problems with linear regression-based methods. The only features that are potentially highly correlated are basal areas and overtopping basal areas specific to uncommon species. For example, the basal area of cottonwood and the overtopping basal area of cottonwood are approximately 95% correlated in the training data, because the only cottonwoods observed came from a few cottonwood filled plots.

```{r spotChecks, cache=TRUE}
invisible({capture.output({
  # train in parallel to speed
  library(doParallel)
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  # generalized linear model
  set.seed(seed)
  fit.glmnet <- train(dbh_recipe, trainsub, method = 'glmnet', metric=metric,
                      trControl=control)
  # neural network (one layer)
  set.seed(seed)
  fit.nnet <- train(dbh_recipe, trainsub, method = 'nnet', metric=metric,
                    trControl=control)
  # neural network (multilayer perceptitron)
  set.seed(seed)
  fit.mlp <- train(dbh_recipe, trainsub, method = 'mlp', metric=metric,
                   trControl=control)
  # linear support vector machine
  set.seed(seed)
  fit.svmLinear <- train(dbh_recipe, trainsub, method = 'svmLinear', metric=metric,
                         trControl=control)
  # radial support vector machine
  set.seed(seed)
  fit.svmRadial <- train(dbh_recipe, trainsub, method = 'svmRadial', metric=metric,
                         trControl=control)
  # k-nearest neighbor
  set.seed(seed)
  fit.knn <- train(dbh_recipe, trainsub, method = 'knn', metric=metric,
                   trControl=control)
  # cart (trees)
  set.seed(seed)
  fit.cart <- train(dbh_recipe, trainsub, method = 'rpart', metric=metric,
                    trControl=control)
  # bagged trees
  set.seed(seed)
  fit.bagcart <- train(dbh_recipe, trainsub, method = 'treebag', metric=metric,
                       trControl=control)
  # random forest
  set.seed(seed)
  fit.rf <- train(dbh_recipe, trainsub, method = 'ranger', metric=metric,
                  trControl=control)
  # gradient boosting machine
  set.seed(seed)
  fit.gbm <- train(dbh_recipe, trainsub, method = 'gbm', metric=metric,
                   trControl=control, verbose=FALSE)
  # linear regression
  set.seed(seed)
  fit.lm <- train(dbh_recipe, trainsub, method = 'lm', metric=metric,
                  trControl=control)
  # penalized linear regression SLOW!
  set.seed(seed)
  fit.penalized <- train(dbh_recipe, trainsub, method = 'penalized', metric=metric,
                         trControl=control)
  # multivariate adaptive regression splines
  set.seed(seed)
  fit.earth <- train(dbh_recipe, trainsub, method = 'earth', metric=metric,
                     trControl=control)
  # cubist model tree SLOW!
  set.seed(seed)
  fit.cubist <- train(dbh_recipe, trainsub, method = 'cubist', metric=metric,
                      trControl=control)
  # regularized random forest WICKED SLOW! (AND NOT WORTH THE FUSS)
  # set.seed(seed)
  # fit.rrf <- train(dbh_recipe, trainsub, method = 'RRFglobal', metric=metric,
  #                  trControl=control)
  # ridge regression
  set.seed(seed)
  fit.ridge <- train(dbh_recipe, trainsub, method = 'ridge', metric=metric,
                     trControl=control)
  # quantile regression with LASSO penalty
  set.seed(seed)
  fit.lasso <- train(dbh_recipe, trainsub, method = 'rqlasso', metric=metric,
                     trControl=control)
  # bayesian ridge regression SLOW!
  set.seed(seed)
  fit.bridge <- train(dbh_recipe, trainsub, method = 'bridge', metric=metric,
                      trControl=control, verb = 0)
  # elastic net
  set.seed(seed)
  fit.enet <- train(dbh_recipe, trainsub, method = 'enet', metric=metric,
                    trControl=control)
  
  stopCluster(cl)
})})
```

```{r compileSpots, cache=TRUE}
fits <- list(
  glmnet = fit.glmnet,
  nnet = fit.nnet,
  mlp = fit.mlp,
  svmLinear = fit.svmLinear,
  svmRadial = fit.svmRadial,
  knn = fit.knn,
  cart = fit.cart,
  bagcart = fit.bagcart,
  rf = fit.rf,
  gbm = fit.gbm,
  lm = fit.lm,
  penalized = fit.penalized,
  mars = fit.earth,
  cubist = fit.cubist,
  # rrf = fit.rrf,
  ridge = fit.ridge,
  lasso = fit.lasso,
  bridge = fit.bridge,
  enet = fit.enet
  )

results <- resamples(fits)
sizes <- sapply(1:length(fits), function(i) {
  lobstr::obj_size(fits[[i]]$finalModel)
})
predtimes <- lapply(1:length(fits), function(j) {
  times <- sapply(1:30, function(k) {
    system.time(predict(fits[[j]],
                        newdata = train[sample(1:nrow(train), 1000,
                                               replace = F), ]))[[3]]
  })
  return(data.frame(model = rep(names(fits)[[j]], 30),
                    time = times))
})
predtimes <- do.call(rbind, predtimes)
```

# Spot-Checks

A diverse set of `r length(fits)` algorithms was successfully spot-checked using ten-fold cross validation, with root mean square error (RMSE) as the evaluation metric. Spot-checking was designed to quickly identify the most promising types of algorithms, and default parameters were used. 

``` {r spotResults, fig.cap="Accuracies of spot-checked algorithms, using three metrics. Accuracies were estimated using repeated cross validation.", cache=FALSE}
bwplot(results, scales = scales)
```

```{r spotPredtimes, fig.cap="Prediction times for spot-checked algorithms, based on 30 random samples of 1,000 trees each.", cache=FALSE}
predtimes %>%
  mutate(model = fct_reorder(factor(model), time, median, .desc = T)) %>%
  ggplot(aes(model, time)) +
  geom_boxplot() +
  coord_flip() +
  scale_y_continuous(name = "seconds to make 1,000 predictions")
```

```{r spotSizes, fig.cap="Sizes of spot-checked model objects.", cache=FALSE}
data.frame(model = names(fits), size = sizes) %>%
  mutate(size = size / 1000000,
         model = fct_reorder(factor(model), size, .desc = T)) %>%
  ggplot(aes(model, size)) +
  geom_point(size = 2) +
  geom_segment( aes(x=model, xend=model, y=0, yend=size)) +
  coord_flip() +
  scale_y_continuous(name = "model size (megabytes)")
```

The spot-checking demonstrated that non-linear methods like model trees, random forests, gradient boosting machines, and support vector machines tend to be the most accurate in this case. Regularized parametric models are not quite as accurate, but they tend to be smaller and faster at making predictions. 

Of the non-linear models, the gradient boosting machine (gbm) may be the best compromise, offering high accuracy with relatively short prediction times in a smallish package. It also looks like its hyperparameters were not optimized well, and there could be further accuracy gains from revisiting them. Gradient boosting machines tend to be sensitive to their hyperparameters, and the default grid search that was conducted may have been insufficient. The following plot shows that no minimum RMSE was found in the default search. Increasing the number of boosting iterations and the maximum tree depth could lower RMSE values.

``` {r gbmplot}
plot(fit.gbm)
```

# Final Model

In order to optimize its accuracy, the gradient boosting machine was re-tuned following the process outlined by Boehmke and Greenwell in _Hands-On Machine Learning with R_ (2020, section 12.3.3). Unfortunately, the tuning depends on the size of the training data, so all of the training data had to be used, which makes the process very slow. Five-fold cross validation was used instead of 10-fold to decrease the training time.

```{r gbmParam}
# faster control procedure because this is comp heavy 
control2 <- trainControl(method = "cv", number = 5)

# First find best number of boosting iterations (trees) at high learning rate
invisible({capture.output({
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.gbm1 <- train(dbh_recipe, train, method = 'gbm', metric=metric,
                    trControl=control2, verbose=FALSE,
                    tuneGrid = expand.grid(shrinkage = .5,
                                           n.trees = seq(14000, 26000, by = 3000),
                                           interaction.depth = 3, 
                                           n.minobsinnode = 10)) 
  
  stopCluster(cl)
})})

plot(fit.gbm1)
```

```{r gbmTrees}
# Then optimize tree-specific parameters 
invisible({capture.output({
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.gbm2 <- train(dbh_recipe, train, method = 'gbm', metric=metric,
                   trControl=control2, verbose=FALSE,
                   tuneGrid = expand.grid(shrinkage = fit.gbm1$bestTune$shrinkage,
                                          n.trees = fit.gbm1$bestTune$n.trees,
                                          interaction.depth = c(2, 4, 6, 8),
                                          n.minobsinnode = c(5, 10, 15)))
  
  stopCluster(cl)
})})

plot(fit.gbm2)
```

```{r gbmLR}
# fit.gbm2 TOOK SEVERAL DAYS. LOWERING LEARNING RATE WOULD BE EVEN SLOWER.
# LEAVE THIS PART OUT.

# Now can try lower learning rate (shrinkage) and retrain boosting iterations
# invisible({capture.output({
# cl <- makePSOCKcluster(10)
# registerDoParallel(cl)
# 
# set.seed(seed)
# fit.gbm3 <- train(dbh_recipe2, train, method = 'gbm', metric=metric,
#                  trControl=control2, verbose=FALSE,
#                  tuneGrid = expand.grid(shrinkage = 0.01,
#                                         n.trees = c(20000, 30000, 40000),
#                                         interaction.depth = 3,
#                                         n.minobsinnode = 15))
# 
# stopCluster(cl)
# })})
# 
# plot(fit.gbm3)
# 
# invisible({capture.output({
# cl <- makePSOCKcluster(10)
# registerDoParallel(cl)
# 
# set.seed(seed)
# fit.gbm4 <- train(dbh_recipe2, train, method = 'gbm', metric=metric,
#                  trControl=control2, verbose=FALSE,
#                  tuneGrid = expand.grid(shrinkage = 0.01,
#                                         n.trees = c(80000, 120000),
#                                         interaction.depth = 3,
#                                         n.minobsinnode = 15))
# 
# stopCluster(cl)
# })})
# 
# plot(fit.gbm4)
```

``` {r gbmFinal}
# Refit without resampling, to get smaller train object
invisible({capture.output({
  cl <- makePSOCKcluster(10)
  registerDoParallel(cl)
  
  set.seed(seed)
  fit.gbm.final <- 
    train(dbh_recipe, train, method = 'gbm', metric=metric,
          trControl = trainControl(method = "none"), verbose = FALSE,
          tuneGrid = expand.grid(shrinkage = fit.gbm2$bestTune$shrinkage,
                                 n.trees = fit.gbm2$bestTune$n.trees,
                                 interaction.depth = 
                                   fit.gbm2$bestTune$interaction.depth,
                                 n.minobsinnode = 
                                   fit.gbm2$bestTune$n.minobsinnode))
  
  stopCluster(cl)
})})
```

```{r gbmTimes}
gbmTimes <- 
  sapply(1:30, function(k) {
    system.time(predict(fit.gbm.final,
                        newdata = train[sample(1:nrow(train), 1000,
                                               replace = F), ]))[[3]]
  })
```

This final version of the gradient boosting machine achieves a cross-validation RMSE of `r round(min(fit.gbm2$results[,5]), 4)` inches. It uses `r round(lobstr::obj_size(fit.gbm.final)/1000000)` megabytes in memory, and takes an average of `r round(mean(gbmsubTimes), 2)` seconds to make one thousand predictions.

``` {r save}
saveRDS(fit.gbm.final, file = "dbh-growth-model-gbm.rds")
```

